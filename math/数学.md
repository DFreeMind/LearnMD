# 数学

## 线性代数

### 概念
#### 矩阵基础概念

**单位矩阵**：矩阵对角线值都为 1 ，其他元素值都为零的方阵，，记做 $\mathbf I$，如 3 × 3 的单位矩阵：
$ \mathbf I = 
\left[\begin{matrix} 
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & 1
\end{matrix} \right ]
$

**单位向量（矢量）**：$\mathbf L^2$ 范数为 $\mathbf 1$ 的向量，如 $ 
\mathbf a = 
\left [ \begin{matrix} 
\frac{\sqrt{2}}{2} \\ 
\frac{\sqrt{2}}{2}
\end{matrix} \right ]
$

**逆矩阵**：对于n 阶方阵 A，若存在一个 n 阶矩阵 B 使得$\mathbf{AB = BA = E}$，那么 B 称作 A 的逆矩阵，记做 $\mathbf{A^{-1}=B}$，称 A 为可逆矩阵，

**对角矩阵**：除了对角线之外，其他元素全为零，单位矩阵就是一种特殊的对角矩阵，如 
$$
\mathbf D = 
\left [\begin{matrix}
1 & 0 & 0 & 0 \\
0 & 2 & 0 & 0 \\
0 & 0 & 4 & 0 \\
0 & 0 & 0 & 7 
\end{matrix} \right ]
$$
也可以记做 $diag(\mathbf v)$，其中 $\mathbf v$ 为 $[1,2,4,7]^\mathbf T$
当紧当对角线的值不为零时，矩阵存在逆矩阵，即$diag(v)^{-1} = diag(\left[\frac{1}{v_1}, \cdots, \frac{1}{v_n} \right]^\mathbf T) $。在进行计算时，就是将相应的 x 值进行缩放，即 $diag(\mathbf v) \mathbf x = \mathbf v \bigodot \mathbf x$


**对称矩阵**：转置等于自身，如通过距离函数生成的矩阵，点(i,j)与点(j,i)到远点的距离是相等的，可以表示为，$\mathbf A = \mathbf A^\mathbf T $。若矩阵中所有的值都是实数，那么就称此矩阵为*实对称矩阵*，实对称矩阵有以下几个特性：
- 实对称矩阵A的不同特征值对应的特征向量是正交的。
- 实对称矩阵A的特征值都是实数，特征向量都是实向量。
- n阶实对称矩阵A必可对角化，且相似对角阵上的元素即为矩阵本身特征值。

**正交向量**：当 $\mathbf x^\mathbf T \mathbf y = 0$ 时，两向量正交。当两个向量都为非零向量时，两向量夹角为 90°。当两个向量的范数都为 1 的话，就说两向量是标准正交的（正规化）。

**正交矩阵**：当矩阵的行互相正交，且列也互相正交，那么就说这个矩阵是一个正交矩阵，即 
$\mathbf A^\mathbf T \mathbf A = \mathbf A \mathbf A^\mathbf T = \mathbf I$，从中可以看到 $\mathbf A^{-1} = \mathbf A^{T}$

#### 矩阵特征

**特征向量、特征值**：矩阵 $\mathbf A $ 的特征向量$\mathbf v $可以表示为 $\mathbf A \mathbf v = \lambda \mathbf v$，其中 $\lambda$ 为特征值。
- 当一个矩阵的所有特征值都是正数时，称此矩阵是*正定的（positive definite）*，可以保证 $\mathbf{x^{T}Ax = 0 \Rightarrow x = 0}$
- 当特征值全是正数或者0时，称此矩阵是*半正定的（positive semideﬁnite）*，可以保证$\mathbf{\forall x,  x^{T}Ax \geqslant 0}$

同理，当特征值都是负值时称为*负定矩阵（negative deﬁnite）*，当特征值都是负值或者0时称为*半负定矩阵（negative  semideﬁnite）*

**特征值分解**：*只能用于方阵*，假设矩阵 $\mathbf A$ 是由 n 个线性独立的特征值组成 $\{\mathbf v^{(1)},\cdots,\mathbf v^{(n)} \}$，其中特征值为 $\{\mathbf \lambda_{1},\cdots,\mathbf \lambda_{n} \}$，把所有的特征向量连接成矩阵$\mathbf V=\{\mathbf v^{(1)},\cdots,\mathbf v^{(n)} \}$, 所有的特征值连接成$\mathbf λ = \{\mathbf \lambda_{1},\cdots,\mathbf \lambda_{n} \}$，那么矩阵 $\mathbf A$ 的特征值分解可以表示为: $$\mathbf A = \mathbf V diag(λ) \mathbf V^{-1}$$

**奇异矩阵**：奇异矩阵是对方阵来说的，当方阵的行列式|A| 或者任何特征值都为 0 时，就是奇异矩阵，不为 0 则为非奇异矩阵。

**奇异值分解（SVD Singular Value Decomposition）**：另一种分解矩阵的方式，将矩阵分解成奇异值（singular values）与 奇异向量（singular vectors）。每个实数矩阵都可以进行奇异值分解，但不一定可以进行特征值分解。奇异值分解可以写作$$\mathbf A = \mathbf{UDV^{T}}$$
若 A 是一个 m×n 的矩阵，那么 U 是一个 m×m 的矩阵，D 是一个 m×n 的矩阵，T 是一个 n×n 的矩阵。其中 U、V 需要时正交矩阵，D 是一个对角矩阵，但可以不是方阵。

D 对角线上的元素成为矩阵 A 的奇异值，U 的列成为 A 的左奇异向量（left-singular vectors），V 的列是 A 的右奇异向量（right-singular vectors）。

**矩阵的迹（Trace）**：n 阶方阵 A 主对角线上元素的和成为矩阵的迹，记做 $\mathbf{tr(A) = \sum\limits_{i} A_{i,i}}$。标量的迹是她自己$a = tr(a)$。
- $\mathbf{tr(A) = tr(A^{T})}$
- 循环特性(矩阵的维度相关)：$\mathbf{tr(ABC) = tr(CAB) = tr(BCA)}$，更一般的写法：$\mathbf{tr(\prod\limits_{i=1}^{n}F^{(i)}) = tr(F^{(n)}\prod\limits_{i=1}^{n-1}F^{(i)})}$
- 在维度不相关时一样有：$\mathbf{tr(AB) = tr(BA)}$

**行列式(determinate)** ：记做$\mathbf{det(A)}$ ，其值等于矩阵所有特征值的乘积。

### 基础
- [deep learning book linear_algebra](http://www.deeplearningbook.org/contents/linear_algebra.html#pf6)
- [斯坦福线性代数 - Linear Algebra Review and Reference](https://see.stanford.edu/materials/aimlcs229/cs229-linalg.pdf)
- [矩阵参考手册-The Matrix CookBook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)

### 求导
求导法则：
$$
\begin{eqnarray}
\frac{\partial y x}{\partial x}=y^T \\
\frac{\partial(x^TA x)}{\partial x}=(A+A^T)x \\
\end{eqnarray}
$$


- [机器学习中的线性代数之矩阵求导](https://blog.csdn.net/u010976453/article/details/54381248)

- [向量的L2范数求导](https://www.cnblogs.com/nowgood/p/fanshuqiudao.html)

- [wikipedia - Matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus)

## 概论
- [Probability and InformationTheory](http://www.deeplearningbook.org/contents/prob.html)
- [Jaynes Probability Theory](http://www.med.mcgill.ca/epidemiology/hanley/bios601/GaussianModel/JaynesProbabilityTheory.pdf)

### 相关概念
- 置信度（degree of belief）
- 频率论概论（frequentist probability）
- 贝叶斯概论（Bayesian probability）
- 中心极限定理 (central limit theorem)
- 潜在变量（Latent Variable）
- 高斯混合模型（Gaussian mixture model）
- 先验概率（prior probability）
- 后验概率（posterior probability）

### 基础
#### 离散变量
可以使用离散概率质量函数（probability mass function PMF）描述，通过记做大写的 $P$，每一个 $P$ 都会关联一个随机变量，如 $P(x)$ 不同 $P(y)$.

通常 $X=x$ 的概率记做 $P(x)$ ，值为 1 表示 $X=x$ 是确定的，值为 0 表示 $X=x$ 是不可能的。

为了消除歧义，以上通常写作 $P(X=x)$。 有时也会先定义变量$X$，然使用 $\thicksim$ 在后面跟上他符合的分布：$X \thicksim P(X)$

**联合概率分布（joint probabilitydistribution）**：$P(X=x, Y=y)$ 或者简记为 $P(x,y)$

**P应满足的条件**：
- P的阈值必须是 X 的所有可能值
- $\forall x \in X , \qquad 0 \leq P(x) \leq 1$
- $\sum_{x \in X}P(x) = 1$，此特性被称为正规化（normalized）

**均匀分布**：让离散随机变量 X 有 k 个不同的状态，在 X 上的均匀分布的 PMF 为: $P(X=x_{i})=\frac{1}{k} $，可以得到 $\sum_{i}P(X=x_{i})=\sum_{i}\frac{1}{k} = \frac{k}{k} = 1$，即分布式正规划的

#### 连续变量
连续变量使用概率密度函数（ probability density function PDF）来描述，而不是概率质量函数（PMF）。此时的 $p$ 应该满足一下的条件：
- $p$ 的阈值必须是 X 的所有可能值
- $\forall x \in X , \qquad p(x) \geq 0$ ，但不需要 $p(x) \leq 1$ 的限制
- $\int p(x)dx = 1$

概率密度函数并没有直接给出某个状态的概率，而是通过一个极小的区域 $\delta x$ 与 $p(x)$ 的乘积给出：$p(x)\delta x$。通过积分可以求出某一点具体的概率，特别是 x 在某个区域上，可通过以下公式求出：$\int_{[a,b]}p(x)dx$

一种特殊的概率密度分布，在实数上间隔的均匀分布 $u(x;a,b)$ ，其中 a、b 是就间隔的端点，且 b > a，";" 表示被参数化。为了保证间隔不在端点之外令 $\forall x \notin [a,b] \qquad u(x;a,b)=0$，在 [a,b] 以内有 $u(x;a,b)=\frac{1}{b-a}$，可以看到其积分值也为 1 。x 在[a,b]范围内服从均匀分布可以记为 $X \thicksim U(a,b)$

#### 边缘概率(marginal probability)

当知道了一组变量的概率分布之后，想要求变量子集的分布，此种分布就成为边缘概率分布（marginal probability distribution）。

如对于离散随机变量X、Y，知道他们的联合概率分布 P(X,Y)，那么通过求和法则就可以找到 P(X)，$\forall x \in X, P(X=x)=\sum_{y}P(X=x,Y=y)$，对于连续的则需要改成积分形式 $p(x) = \int p(x,y)dy$

#### 条件概率（Conditional Probability）

通过已知事件来求另一些时间的概率叫做条件概率。即在 X=x 的条件下 Y=y 的概率记做：$P(Y=y | X=x)$。可以通过以下的方式计算：$P(Y=y | X=x)=\frac{P(X=x,Y=y)}{P(X=x)}$

#### 条件概率的链式法则
很多变量的联合概率分布可以分解为只有一个变量的条件概率，即：$P(X^{(1)},\cdots,X^{(n)})=P(X^{(1)})\prod_{i=2}^{n} P(X^{(i)}|X^{(1)},\cdots,X^{(i-1)})$ 。这个被称为链式法则或者乘法法则。

如下示例，使用两次上述法则：
$$P(a,b,c)=P(a|b,c)P(b,c) \tag{1}$$
$$P(b,c)=P(b|c)P(c) \tag{2}$$
$$P(a,b,c)=P(a|b,c)P(b|c)P(c) \tag{3}$$

#### 独立与条件独立
**独立**：两个随机变量x、y独立可以写成如下形式，$\forall x \in X,y \in Y, P(X=x,Y=y)=P(X=x)P(Y=y)$，可以记为 $X \perp Y$

**条件独立**：在给定的变量 z 的条件下，若随机变量 x、y 可以写成如下形式：
$$\forall x \in X, y \in Y, z \in Z, P(X = x, Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z)$$
可以记为 $X \perp Y | Z$

#### 期望（Expectation）、方差（Variance）、协方差（Covariance）
**期望**

关于概率分布 P(X) 的函数 $f(x)$ 的期望是函数 $f$ 作用在来自于 P 的 x，离散情况计算如下：
$$\mathbb{E}_{X \thicksim P}[f(x)] = \sum_{x}P(x)f(x)$$
连续的变量计算如下：
$$\mathbb{E}_{X \thicksim P}[f(x)] = \int p(x)f(x)dx$$

当 P 明确时可以简记成如下 $\mathbb{E}_{x}[f(x)]$，当 X 与 P 都明确的情况下，上述可以简记成如下形式 $\mathbb{E}[f(x)]$.

期望是线性的，当 α与β不宜懒与 x 时，即有如下形式：
$$\mathbb{E}[\alpha f(x) + \beta g(x)] = \alpha \mathbb{E}[f(x)] + \beta \mathbb{E}[g(x)]$$

**方差**

公式如下：
$$Var(f(x)) = \mathbb{E}[(f(x) − \mathbb{E}[f(x)])^{2}]$$

方差越小，f(x) 越接近期望值。方差的平方根称作标准差（standard variance）。

**标准差**

标准差表示数据波动的情况，标准差越小数据越集中，标准差越大数据波动越大。


**协方差**

给出了两个值得相关性，可以表示成如下形式：
$$Cov(f(x), g(y)) = \mathbb{E} [(f(x) − \mathbb{E} [f(x)]) (g(y) − \mathbb{E} [g(y)])] $$

协方差越大表明两个变量的相关性越低。

两个独立的变量，协方差为 0 ；当协方差不为 0 时，两个变量相关。

**协方差矩阵**

对于一个随机向量 $x \in \mathbb{R}^{n}$，他的协方差矩阵是一个 n×n 的方阵：$Cov(X)_{i,j}= Cov(x_i, x_j)$。对角线上的元素是方差：$Cov(x_i, x_i) = Var(x_i)$

各向同性（isotropic）协方差矩阵表示各个方向的方差相同；对角协方差矩表示可以分别控制轴对齐方向的方差；满秩协方差矩阵表示可以沿着任意方向控制方差，如下图：![image](//wx3.sinaimg.cn/large/69d4185bly1ftkqwuzw1dj20jm0g8jta.jpg)


#### 贝叶斯公式
$$P(X|Y)=\frac{P(X)P(Y|X)}{P(Y)}$$
其中 P(Y) 可以通过如下公式计算：
$$P(Y) = \sum_{x} P(Y|x)P(x)$$

### 常见的概率分布
#### 伯努利分布（Bernoulli Distribution）
是一个单二进制变量分布（single binary random variable），通过参数 $\phi \in [0,1]$ 来控制，具有以下的属性
- $P(X=1) = \phi$
- $P(X=0) = 1 - \phi$
- $P(X=x) = \phi^{x}(1 - \phi)^{1-x}$
- $\mathbb{E}_x[x] = \phi$
- $Var_x(x) = \phi(1 − \phi)$

#### 正态分布（高斯分布）
关于实数的分布，也是最常用的分布，公式如下：
$$\mathcal{N(x;\mu,\sigma^{2})}=\sqrt{\frac{1}{2\pi\sigma^{2}}}exp\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)$$

其中 $\mu \in \mathbb{R},\; \sigma \in (0, \infty)$，μ 是中间峰值的高度，同时也是期望值，即 $\mathbb E[x] = µ$。标准差是 $\sigma$ ，方差是 $\sigma^{2}$

**多元正态分布**

$$\mathcal{N(x;\mathbf{\mu,\Sigma})}=\sqrt{\frac{1}{(2\pi)^{n}det(\Sigma)}}exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$$



## 信息论
- [Probability and InformationTheory](http://www.deeplearningbook.org/contents/prob.html)

----
- [Elements of information theory](http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf)

从不太可能发生的事情中了解到的信息比可能发生的事情多。

定义事件 X=x 的自信息（self-information）为 ：
$$I(x) = -logP(x)$$

其中 log 是以 e 为底的自然对数，I 的单位为 纳特（nats），一纳特的信息量是通过观察的概率 $\frac{1}{e} 得到$。当以 2 为底数时，I 的单位为比特（bits）或者香农（shannons）。

## 数值计算
- [deep learning book - numerical](http://www.deeplearningbook.org/contents/numerical.html)
- [矩阵的条件数（condition number）](https://blog.csdn.net/lanchunhui/article/details/51372831)


### 概念
- 偏导数（partial derivatives）
- 牛顿法（Newton’s method）
- 李普希茨连续函数（Lipschitz continuous）
- 数值梯度（numerical gradient）【近似但简单，速度慢】
- 分析梯度（analytic gradient）【速度快，容易出错】
- 中心差分公式（centered difference formula）

### 导数
**反向传播**

- [Backpropagation, Intuitions](http://cs231n.github.io/optimization-2/)
- [Vector, Matrix, and Tensor Derivatives - Erik Learned-Miller](http://cs231n.stanford.edu/vecDerivs.pdf)

### 偏导数
常用于多变量求导，$\frac{\partial}{\partial x_i}f(\mathbf x)$ 表示在 $\mathbf x$ 点的变量 $x_i$ 增加是 $f$ 改变了多少。对于向量，f 的导数也是向量，其中包含了所有的偏导，可以记做 $\nabla_x f(x)$。

在高维时，临界点（critical points）是每个元素的梯度都为零的点。

在 $u$ 方向导数（directional derivate）是 f 在 $u$ 方向上的斜率。也就是说，方向导数是关于$\alpha$的函数$f(x + \alpha u)$的方向导数。通过链式法则对$\alpha$求导$\frac{\partial}{\partial \alpha}f(x + \alpha u)$ 得到 $u^{T}\nabla_x f(x)$ 在 $\alpha = 0$时得到最小值。


#### 雅各比（Jacobian）与黑赛（Hessian）矩阵
- [百科-雅可比矩阵](https://baike.baidu.com/item/%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5)
- [百科-海赛矩阵](https://baike.baidu.com/item/%E9%BB%91%E5%A1%9E%E7%9F%A9%E9%98%B5/2248782)

在向量分析中，雅可比矩阵是函数的一阶偏导数以一定方式排列成的矩阵，其行列式称为雅可比行列式。

如果我们有一个函数：$f:\mathbb R^m \to \mathbb R^n$ 那么雅各比矩阵就是$\mathbf J \in \mathbb R^{n × m}$ 其定义形式如下：${J}_{i,j} = \frac{\partial}{\partial x_j}f(\mathbf x)_i$

**二阶导数**

即导数的导数，可以记为$\frac{\partial^2}{\partial x_i \partial x_j}f$，在一维的情况下可以记为：$\frac{d^2}{dx^2}f$ 或者 $f''(x)$。

他告诉了我们，当改变输入的时候一阶导数如何变化。

二阶偏导操作具有交换特性：
$$\frac{\partial^2}{\partial x_i \partial x_j}f(\mathbf x) = \frac{\partial^2}{\partial x_j \partial x_i}f(\mathbf x)$$

**海赛矩阵**

海赛矩阵可以用来表示多维输入二阶导数，其形式如下：
$$\mathbf{H}(f)(\mathbf x)_{i,j} = \frac{\partial^2}{\partial x_i \partial x_j}f(\mathbf x)$$
可知，海赛矩阵是雅各比矩阵的梯度。由二阶导出操作具有可交换性，可知 $H_{i,j} = H_{j,i}$，因此海赛矩阵在此点具有对称性。
![image](//ws1.sinaimg.cn/large/69d4185bly1ftlwwmdoahj20jh0d878d.jpg)

![image](//ws1.sinaimg.cn/large/69d4185bly1ftlx4dkvdqj20jb0g90w2.jpg)
